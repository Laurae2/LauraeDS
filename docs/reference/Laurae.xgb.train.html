<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>xgboost Model Trainer — Laurae.xgb.train â€¢ LauraeDS</title>

<!-- jquery -->
<script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<!-- Font Awesome icons -->
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">


<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script>
<script src="../pkgdown.js"></script>
  
  
<!-- mathjax -->
<script src='https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->


  </head>

  <body>
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">LauraeDS</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
      
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      
      </header>

      <div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>xgboost Model Trainer</h1>
    </div>

    
    <p>Trains an xgboost model. Requires <code>Matrix</code> and <code>xgboost</code> packages.</p>
    

    <pre class="usage"><span class='fu'>Laurae.xgb.train</span>(<span class='no'>train</span>, <span class='kw'>watchlist</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>seed</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>verbose</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>verbose_iterations</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>objective</span> <span class='kw'>=</span> <span class='st'>"reg:linear"</span>, <span class='kw'>metric</span> <span class='kw'>=</span> <span class='st'>"rmse"</span>,
  <span class='kw'>maximize</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>boost_method</span> <span class='kw'>=</span> <span class='st'>"gbtree"</span>, <span class='kw'>boost_tree</span> <span class='kw'>=</span> <span class='st'>"hist"</span>,
  <span class='kw'>boost_grow</span> <span class='kw'>=</span> <span class='st'>"depthwise"</span>, <span class='kw'>boost_bin</span> <span class='kw'>=</span> <span class='fl'>255</span>, <span class='kw'>boost_memory</span> <span class='kw'>=</span> <span class='st'>"uint32"</span>,
  <span class='kw'>boost_weighting</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>learn_threads</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>learn_shrink</span> <span class='kw'>=</span> <span class='fl'>0.05</span>,
  <span class='kw'>iteration_max</span> <span class='kw'>=</span> <span class='fl'>100</span>, <span class='kw'>iteration_trees</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>iteration_stop</span> <span class='kw'>=</span> <span class='fl'>20</span>,
  <span class='kw'>tree_depth</span> <span class='kw'>=</span> <span class='fl'>6</span>, <span class='kw'>tree_leaves</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>sample_row</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>sample_col</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>reg_l1</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>reg_l2</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>reg_l2_bias</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>reg_loss</span> <span class='kw'>=</span> <span class='fl'>0</span>,
  <span class='kw'>reg_hessian</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>dart_rate_drop</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>dart_skip_drop</span> <span class='kw'>=</span> <span class='fl'>0</span>,
  <span class='kw'>dart_sampling</span> <span class='kw'>=</span> <span class='st'>"uniform"</span>, <span class='kw'>dart_norm</span> <span class='kw'>=</span> <span class='st'>"tree"</span>, <span class='kw'>dart_min_1</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='no'>...</span>)</pre>
    
    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a> Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>train</th>
      <td><p>Type: xgb.DMatrix. The training data.</p></td>
    </tr>
    <tr>
      <th>watchlist</th>
      <td><p>Type: list of xgb.DMatrix. The data to monitor through the metrics, defaults to <code>list()</code>.</p></td>
    </tr>
    <tr>
      <th>seed</th>
      <td><p>Type: numeric. Seed for the random number generator for reproducibility, defaults to <code>1</code>.</p></td>
    </tr>
    <tr>
      <th>verbose</th>
      <td><p>Type: numeric. Whether to print messages. Defaults to <code>1</code>.</p></td>
    </tr>
    <tr>
      <th>verbose_iterations</th>
      <td><p>Type: numeric. How many iterations to cool down before printing on the console again. Defaults to <code>1</code>.</p></td>
    </tr>
    <tr>
      <th>objective</th>
      <td><p>Type: character or function. The objective to optimize, defaults to <code>reg:linear</code>.</p><ul>
<li><p><code>"reg:linear"</code>Linear Regression.</p></li>
<li><p><code>"reg:logistic"</code>Logistic Regression.</p></li>
<li><p><code>"binary:logistic"</code>Logistic Regression (binary classification, probabilities).</p></li>
<li><p><code>"binary:logitraw"</code>Logistic Regression (binary classification, raw score).</p></li>
<li><p><code>"multi:softmax"</code>Multiclass Logistic Regression (multiclass classification, best class).</p></li>
<li><p><code>"multi:softprob"</code>Multiclass Logistic Regression (multiclass classification, probability matrix).</p></li>
<li><p><code>"rank:pairwise"</code>LambdaMART-like Ranking (pairwise loss).</p></li>
<li><p><code>"count:poisson"</code>Poisson Regression (count).</p></li>
<li><p><code>"poisson-nloglik"</code>Negative Log Likelihood (Poisson Regression).</p></li>
<li><p><code>"reg:gamma"</code>Gamma Regression with Log-link.</p></li>
<li><p><code>"gamma-nloglik"</code>Negative Log Likelihood (Gamma Regression).</p></li>
<li><p><code>"gamma-deviance"</code>Residual Deviance (Gamma Regression).</p></li>
<li><p><code>"reg:tweedie"</code>Tweedie Regression with Log-link.</p></li>
<li><p><code>"tweedie-nloglik"</code>Negative Log Likelihood (Tweedie Regression).</p></li>
</ul></td>
    </tr>
    <tr>
      <th>metric</th>
      <td><p>Type: character or function. The metric to print against the <code>watchlist</code>, defaults to <code>rmse</code>.</p><ul>
<li><p><code>"rmse"</code>Root Mean Squared Error.</p></li>
<li><p><code>"mae"</code>Mean Absolute Error.</p></li>
<li><p><code>"logloss"</code>Negative Log Likelihood.</p></li>
<li><p><code>"error"</code>Binary classification Error Rate.</p></li>
<li><p><code>"error@t"</code>Binary classification Error Rate at <code>t</code>.</p></li>
<li><p><code>"merror"</code>Multiclass classification Error Rate.</p></li>
<li><p><code>"mlogloss"</code>Multiclass Negative Log Likelihood.</p></li>
<li><p><code>"auc"</code>Area Under the Curve.</p></li>
<li><p><code>"ndcg@n"</code>Normalized Discounted Cumulative Gain at <code>n</code>.</p></li>
<li><p><code>"map@n"</code>Mean Average Precision at <code>n</code>.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>maximize</th>
      <td><p>Type: logical. Whether to maximize the metric, defaults to <code>NULL</code>.</p></td>
    </tr>
    <tr>
      <th>boost_method</th>
      <td><p>Type: character. Boosting method, defauts to <code>"gbtree"</code>.</p><ul>
<li><p>Boosting Method.</p></li>
<li><p>xgboost has access to three different boosting methods:</p><ul>
<li><p><code>"gblinear"</code>Generalized Linear Model, which is using Shotgun (Parallel Stochastic Gradient Descent).</p></li>
<li><p><code>"gbtree"</code>Gradient Boosted Trees, which is the default boosting method using Decision Trees and Stochastic Gradient Descent.</p></li>
<li><p><code>"dart"</code>Dropout Additive Regression Trees, which is a method employing the Dropout method from Neural Networks.</p></li>
</ul></li>
<li><p>The booster method has a huge impact on training performance.</p></li>
<li><p>The booster method defines the algorithm you will use for boosting or training the model.</p></li>
<li><p>For instance, a linear boosted model is obviously better for linear problems.</p></li>
<li><p>Tree-based boosted models are better for non-linear problems, as they have the ability to approximate them.</p></li>
<li><p>DART (Dropout Additive Regression Trees) is similar to Dropout in neural networks, except you are applying this idea to trees (dropping trees randomly).</p></li>
</ul></td>
    </tr>
    <tr>
      <th>boost_tree</th>
      <td><p>Type: character. Tree method, defauts to <code>"hist"</code>.</p><ul>
<li><p>Tree Method.</p></li>
<li><p>Tips: leave it alone unless you know what you are doing.</p></li>
<li><p>This parameter is exclusive to xgboost implementation and three different values:</p><ul>
<li><p><code>"exact"</code>for training the exact original xgboost.</p></li>
<li><p><code>"approx"</code>for training the approximate/distributed xgboost.</p></li>
<li><p><code>"hist"</code>for training xgboost in fast histogram mode, similarly to LightGBM.</p></li>
</ul></li>
<li><p>The tree method has a huge impact on training speed.</p></li>
<li><p>The way trees are built is essential to maximize or lower performance for training.</p></li>
<li><p>In addition, it has a huge impact on the training speed, as leaving feature accuracy down for lower passes during training loops allow to train models significantly faster.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>boost_grow</th>
      <td><p>Type: character. Growing method, defauts to <code>"depthwise"</code>.</p><ul>
<li><p>Growing Method.</p></li>
<li><p>Tips: leave it alone unless you know what you are doing.</p></li>
<li><p>The original xgboost uses depthwise growing policy, which unallows growing deeper trees as long as all the nodes are not at the same level.</p></li>
<li><p>The depthwise policy (<code>grow_policy = "depthwise"</code>) acts as a regularizer which lowers the fitting performance, by providing a potentially higher generalization performance.</p></li>
<li><p>To act as the same as LightGBM (growing at the best loss instead of at the best depth), set <code>grow_policy = "lossguide"</code>.</p></li>
<li><p>The tree growing method allows to switch between two ways of training:</p><ul>
<li><p>depth-wise method: original xgboost training way, which is highly performing on datasets not relying on distribution rules (far from synthetic).</p></li>
<li><p>loss-guide method: original LightGBM training way, which is highly performing on datasets relying on distribution rules (close to synthetic).</p></li>
</ul></li>
<li><p>The xgboost way of training allows to minimize depth, where growing an additional depth is considered as a last resort.</p></li>
<li><p>The LightGBM way of training allows to minimize loss, where growing an additional depth is not considered as a last resort.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>boost_bin</th>
      <td><p>Type: numeric. Maximum number of unique values per feature, defauts to <code>255</code>.</p><ul>
<li><p>Number of maximum unique values per feature.</p></li>
<li><p>Tips: leave it alone unless you know what you are doing.</p></li>
<li><p>xgboost does not optimize the dataset storage depending on the max_bin parameter.</p></li>
<li><p>As such, it requires 4GB RAM to train a model on Higgs 3.5M.</p></li>
<li><p>By providing less unique values per feature, the model can be trained significantly faster without a large loss in performance.</p></li>
<li><p>In cases where the dataset is closer to a synthetic dataset, the model might perform even better than without binning.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>boost_memory</th>
      <td><p>Type: character. Memory used for binning, defauts to <code>"uint32"</code>.</p><ul>
<li><p>Memory pressure of bins.</p></li>
<li><p>Tips: leave it alone unless you know what you are doing.</p></li>
<li><p>The positive label should be the rare label.</p></li>
<li><p>By performing a weight multiplication on the positive label, the model is performing a cost-sensitive training.</p></li>
<li><p>The cost-sensitive training is applied to the booster model which impacts directly the trained models.</p></li>
<li><p>It implies a potential higher performance, especially when it comes to ranking tasks such as for AUC.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>boost_weighting</th>
      <td><p>Type: numeric. Weighting of positive labels, defauts to <code>1</code>.</p><ul>
<li><p>Multiplication applied to every positive label weight.</p></li>
<li><p>Tips: leave it alone unless you know what you are doing.</p></li>
<li><p>The matrix data type is defining the memory pressure in memory, while determining also the maximum number of bins.</p></li>
<li><p>The default binning is 32 bit, which means 255 bins are possible per column.</p></li>
<li><p>Lowering it to 16 bit (127 bins) or 8 bit (63 bins) lowers the maximum number of bins, therefore lowering accuracy and improving memory pressure.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>learn_threads</th>
      <td><p>Type: numeric. Number of threads, defauts to <code>1</code>.</p><ul>
<li><p>Number of threads using for training models.</p></li>
<li><p>Tips: larger data benefit from more threads, but smaller data has reverse benefits.</p></li>
<li><p>Intel CPUs benefit from hyperthreading and you should use the number of logical cores in your computer instead of the number of physical cores.</p></li>
<li><p>The old rationale "number of threads = physical cores" was when multithreading was so poor that the overhead was too large. Nowadays, this is not true for most cases (you would not multithread anymore if this were true).</p></li>
<li><p>Using multithreaded training allows to train models faster.</p></li>
<li><p>This is not always true in the case of small datasets, where training is so fast that the overhead is too large.</p></li>
<li><p>In addition, when using many threads (like 40 on 1Mx1K dataset), be careful of the number of leaves parameter combined with unlimited depth, as it will massively slow down the training.</p></li>
<li><p>To find the best number of threads, you can benchmark manually the training speed by changing the number of threads.</p></li>
<li><p>Choosing the number of threads depends both on your CPU and the dataset. Do not overallocate logical cores.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>learn_shrink</th>
      <td><p>Type: numeric. Learning rate, defauts to <code>0.05</code>.</p><ul>
<li><p>Multiplication performed on each boosting iteration.</p></li>
<li><p>Tips: set this larger for hyperparameter tuning.</p></li>
<li><p>Once your learning rate is fixed, do not change it.</p></li>
<li><p>It is not a good practice to consider the learning rate as a hyperparameter to tune.</p></li>
<li><p>Learning rate should be tuned according to your training speed and performance tradeoff.</p></li>
<li><p>Do not let an optimizer tune it. One must not expect to see an overfitting learning rate of 0.0202048.</p></li>
<li><p>Each iteration is supposed to provide an improvement to the training loss.</p></li>
<li><p>Such improvement is multiplied with the learning rate in order to perform smaller updates.</p></li>
<li><p>Smaller updates allow to overfit slower the data, but requires more iterations for training.</p></li>
<li><p>For instance, doing 5 iteations at a learning rate of 0.1 approximately would require doing 5000 iterations at a learning rate of 0.001, which might be obnoxious for large datasets.</p></li>
<li><p>Typically, we use a learning rate of 0.05 or lower for training, while a learning rate of 0.10 or larger is used for tinkering the hyperparameters.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>iteration_max</th>
      <td><p>Type: numeric. Number of boosting iterations, defauls to <code>1</code>.</p><ul>
<li><p>Number of boosting iterations.</p></li>
<li><p>Tips: combine with early stopping to stop automatically boosting.</p></li>
<li><p>Larger is not always better.</p></li>
<li><p>Keep an eye on overfitting.</p></li>
<li><p>It is better to perform cross-validation one model at a time, in order to get the number of iterations per fold. In addition, this allows to get a precise idea of how noisy the data is.</p></li>
<li><p>When selecting the number of iterations, it is typical to select 1.10x the mean of the number of iterations found via cross-validation.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>iteration_trees</th>
      <td><p>Type: numeric. Averaged trees per iteration, defauls to <code>100</code>.</p><ul>
<li><p>Number of trees per boosting iteration.</p></li>
<li><p>Tips: Do not tune it unless you know what you are doing.</p></li>
<li><p>To achieve Random Forest, one should use sampling parameters to not get identical trees.</p></li>
<li><p>The combination of Random Forest and Gradient Boosting is well-known "not so good" combination.</p></li>
<li><p>In fact, Gradient Boosted Trees is supposed to be an extension of Decision Trees and Random Forest, using mathematical optimization.</p></li>
<li><p>Therefore, it does not make practical sense to use Gradient Boosted Random Forests.</p></li>
<li><p>To achieve a similar performance to Random Forests, one should use a row sampling of 0.632 (.632 Bootstrap) and a column sampling depending on the task.</p></li>
<li><p>For regression, it is recommended to use 1/3 features per tree.</p></li>
<li><p>For classification, it is recommended to use sqrt(number of features)/(number of features) features per tree.</p></li>
<li><p>For other cases, no recommendations are existing.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>iteration_stop</th>
      <td><p>Type: numeric. Number of iterations without improvement before stopping, defauls to <code>20</code>.</p><ul>
<li><p>Number of maximum iterations without improvements.</p></li>
<li><p>Tips: make sure you added a validation dataset to watch, otherwise this parameter is useless.</p></li>
<li><p>Setting early stopping too large risks overfitting by unallowing training to stop due to luck.</p></li>
<li><p>Scale this parameter appropriately with the learning rate (usually: linearly).</p></li>
<li><p>Early Stopping allows to not let a model train until the end when the validation metric is not improving for a specified amount of iterations.</p></li>
<li><p>By keeping this value low enough, boosting will quickly give up training when there is no improvement over time.</p></li>
<li><p>When it is large enough, boosting will refuse to give up training, even though some improvements over the best iteration might be pure luck.</p></li>
<li><p>This value should be called accordingly with the number of iterations.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>tree_depth</th>
      <td><p>Type: numeric. Maximum tree depth, defauls to <code>6</code>.</p><ul>
<li><p>Maximum depth of each trained tree.</p></li>
<li><p>Tips: use unlimited depth when needing deep branched trees.</p></li>
<li><p>Unlimited depth is essential for training models whose branching is one-sided (instead of balanced branching). such as for long chain of features, like 50 to get to the expected real rule.</p></li>
<li><p>Each model trained at each iteration will have that maximum depth and cannot bypass it.</p></li>
<li><p>As the maximum depth increases, the model is able to fit better the training data.</p></li>
<li><p>However, fitting better the training data does not cause 100<!-- % generalization to the validation data. --></p></li>
<li><p>In addition, this is the most sensible hyperparameter for gradient boosting: tune this first.</p></li>
<li><p>xgboost lossguide training allows 0 depth training (unlimited depth).</p></li>
<li><p>The maximum leaves allowed, if depth is not unlimited, is equal to 2^depth - 1 (ex: a maximum depth of 10 leads to a maximum of 1023 leaves)</p></li>
</ul></td>
    </tr>
    <tr>
      <th>tree_leaves</th>
      <td><p>Type: numeric. Maximum tree leaves, defauls to <code>0</code>.</p><ul>
<li><p>Maximum leaves for each trained tree.</p></li>
<li><p>Tips: adjust depth accordingly by allowing a slightly higher depth than the theoretical number of leaves.</p></li>
<li><p>Restricting the number of leaves acts as a regularization in order to not grow very deep trees.</p></li>
<li><p>It also prevents from growing gigantic trees when the maximum depth is large (if not unlimited).</p></li>
<li><p>Each model trained at each iteration will have that maximum leaves and cannot bypass it.</p></li>
<li><p>As the maximum leaves increases, the model is able to fit better the training data.</p></li>
<li><p>However, fitting better the training data does not cause 100<!-- % generalization to the validation data. --></p></li>
<li><p>In addition, this is the second most sensible hyperparameter for gradient boosting: tune it with the maximum depth.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>sample_row</th>
      <td><p>Type: numeric. Row sampling, defauls to <code>1</code>.</p><ul>
<li><p>Percentage of rows used per iteration frequency.</p></li>
<li><p>Tips: adjust it roughly but not precisely.</p></li>
<li><p>Stochastic Gradient Descent is not always better than Gradient Descent.</p></li>
<li><p>The name "Stochastic Gradient Descent" is technically both right and wrong.</p></li>
<li><p>Each model trained at each iteration will have only a specific <!-- % subset of observations requested using subsample. --></p></li>
<li><p>By training over random partitions of the data, abusing the stochastic nature of the process, the resulting model might fit better the data.</p></li>
<li><p>In addition, this is the third most sensible hyperparameter for gradient boosting: tune it with the column sampling.</p></li>
<li><p>Overfitting happens when a combination of seed and very peculiar sampling value (like 0.728472) is used, as it does not make sense.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>sample_col</th>
      <td><p>Type: numeric. Column sampling per tree, defauls to <code>1</code>.</p><ul>
<li><p>Percentage of columns used per iteration.</p></li>
<li><p>Tips: adjust it roughly but not precisely.</p></li>
<li><p>Stochastic Gradient Descent is not always better than Gradient Descent.</p></li>
<li><p>The name "Stochastic Gradient Descent" is technically both right and wrong.</p></li>
<li><p>Each model trained at each iteration will have only a specific <!-- % subset of features requested using subsample. --></p></li>
<li><p>By training over random partitions of the data, abusing the stochastic nature of the process, the resulting model might fit better the data.</p></li>
<li><p>In addition, this is the third most sensible hyperparameter for gradient boosting: tune it with the row sampling.</p></li>
<li><p>Overfitting happens when a combination of seed and very peculiar sampling value (like 0.728472) is used, as it does not make sense.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>reg_l1</th>
      <td><p>Type: numeric. L1 regularization, defauls to <code>0</code>.</p><ul>
<li><p>L1 Regularization for boosting.</p></li>
<li><p>Tips: leave it alone unless you know what you are doing.</p></li>
<li><p>Adding regularization is not always better.</p></li>
<li><p>The regularization scaling is dataset-dependent and weight-dependent.</p></li>
<li><p>Gradient Boosting applies regularization to the nominator of the gain computation.</p></li>
<li><p>In addition, it is added to the numerator multiplicated by the weight of the sample.</p></li>
<li><p>Each sample has its own pair of gradient/hessian, unlike typical gradient descent methods where that statistic pair is summed up for immediate output and parameter adjustment.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>reg_l2</th>
      <td><p>Type: numeric. L2 regularization, defauls to <code>0</code>.</p><ul>
<li><p>L2 Regularization for boosting.</p></li>
<li><p>Tips: leave it alone unless you know what you are doing.</p></li>
<li><p>Adding regularization is not always better.</p></li>
<li><p>The regularization scaling is dataset-dependent and weight-dependent.</p></li>
<li><p>Gradient Boosting applies regularization to the nominator of the gain computation.</p></li>
<li><p>In addition, it is added to the numerator multiplicated by the weight of the sample.</p></li>
<li><p>Each sample has its own pair of gradient/hessian, unlike typical gradient descent methods where that statistic pair is summed up for immediate output and parameter adjustment.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>reg_l2_bias</th>
      <td><p>Type: numeric. L2 Bias regularization (not for GBDT models), defauls to <code>0</code>.</p><ul>
<li><p>L2 Bias Regularization for boosting.</p></li>
<li><p>Tips: leave it alone unless you know what you are doing.</p></li>
<li><p>Adding regularization is not always better.</p></li>
<li><p>The regularization scaling is dataset-dependent and weight-dependent.</p></li>
<li><p>Gradient Boosting applies regularization to the nominator of the gain computation.</p></li>
<li><p>In addition, it is added to the numerator multiplicated by the weight of the sample.</p></li>
<li><p>Each sample has its own pair of gradient/hessian, unlike typical gradient descent methods where that statistic pair is summed up for immediate output and parameter adjustment.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>reg_loss</th>
      <td><p>Type: numeric. Minimum Loss per Split, defauls to <code>0</code>.</p><ul>
<li><p>Prune by minimum loss requirement.</p></li>
<li><p>Tips: leave it alone unless you know what you are doing.</p></li>
<li><p>Adding pruning threshold is not always better.</p></li>
<li><p>Gamma (loss) regularization happens post training (blocks the trees from being kept) unlike Hessian regularization.</p></li>
<li><p>Loss regularization is a direct regularization technique allowing the model to prune any leaves which do not meet the minimal gain to split criteria.</p></li>
<li><p>This is extremely useful when you are trying to build deep trees but trying also to avoid building useless branches of the trees (overfitting).</p></li>
</ul></td>
    </tr>
    <tr>
      <th>reg_hessian</th>
      <td><p>Type: numeric. Minimum Hessian per Split, defauls to <code>1</code>.</p><ul>
<li><p>Prune by minimum hessian requirement.</p></li>
<li><p>Tips: leave it alone unless you know what you are doing.</p></li>
<li><p>Adding pruning threshold is not always better.</p></li>
<li><p>Hessian regularization happens on the fly (blocks the trees for growing) unlike Loss regularization.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>dart_rate_drop</th>
      <td><p>Type: numeric. DART booster tree drop rate, defauls to <code>0</code>.</p><ul>
<li><p>Probability to to drop a tree on one iteration.</p></li>
<li><p>Tips: leave it alone unless you know what you are doing.</p></li>
<li><p>Smaller/Larger is not always better.</p></li>
<li><p>Defines the dropping probability of each tree during each DART iteration to regenerate gradient/hessian statistics.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>dart_skip_drop</th>
      <td><p>Type: numeric. DART booster tree skip rate, defauls to <code>0</code>.</p><ul>
<li><p>Probability to to skipping any drop on one iteration.</p></li>
<li><p>Tips: leave it alone unless you know what you are doing.</p></li>
<li><p>Smaller/Larger is not always better.</p></li>
<li><p>Defines the probability of skipping dropping during each DART iteration to regenerate gradient/hessian statistics.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>dart_sampling</th>
      <td><p>Type: character. DART booster sampling distribution, defauls to <code>"uniform"</code>. Other choice is <code>"weighted"</code>.</p><ul>
<li><p>Uniform weight application for trees.</p></li>
<li><p>Tips: leave it alone unless you know what you are doing.</p></li>
<li><p>Use <code>sample_type = "uniform"</code> to setup uniform sampling for dropped trees.</p></li>
<li><p>You may use also <code>sample_type = "weighted"</code> to drop trees in proportion to their weights, defined by normalize_type.</p></li>
<li><p>Smaller/Larger is not always better.</p></li>
<li><p>Defines the probability of skipping dropping during each DART iteration to regenerate gradient/hessian statistics.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>dart_norm</th>
      <td><p>Type: character. DART booster weight normalization, defauls to <code>"tree"</code>. Other choice is <code>"forest"</code>.</p><ul>
<li><p>Weight normalization method for trees.</p></li>
<li><p>Tips: leave it alone unless you know what you are doing.</p></li>
<li><p>Smaller/Larger is not always better.</p></li>
<li><p>Normalizing the weight of trees differently allows to put an emphasis on the earliest/latest trees built, leading to different tree structures.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>dart_min_1</th>
      <td><p>Type: numeric. DART booster drop at least one tree, defauls to <code>0</code>. Other choice is <code>1</code>.</p><ul>
<li><p>Minimum of one dropped tree at any iteration.</p></li>
<li><p>Tips: leave it alone unless you know what you are doing.</p></li>
<li><p>Smaller/Larger is not always better.</p></li>
<li><p>Dropping at least one tree at each iteration allows to build different trees.</p></li>
</ul></td>
    </tr>
    <tr>
      <th>...</th>
      <td><p>Other parameters to pass to xgboost's <code>params</code>.</p></td>
    </tr>
    </table>
    
    <h2 class="hasAnchor" id="value"><a class="anchor" href="#value"></a>Value</h2>

    <p>The xgboost model.</p>
    
    <h2 class="hasAnchor" id="details"><a class="anchor" href="#details"></a>Details</h2>

    <p>The following parameters were removed the following reasons:</p><ul>
<li><p><code>debug_verbose</code>was a parameter added to debug Laurae's code for several xgboost GitHub issues.</p></li>
<li><p><code>colsample_bylevel</code>is significantly weaker than <code>colsample_bytree</code>.</p></li>
<li><p><code>sparse_threshold</code>is a mysterious <code>"hist"</code> parameter.</p></li>
<li><p><code>max_conflict_rate</code>is a <code>"hist"</code> specific feature bundling parameter.</p></li>
<li><p><code>max_search_group</code>is a <code>"hist"</code> specific feature bundling parameter.</p></li>
<li><p><code>base_margin</code>is an unusual hyperparameter which should be used for guaranteeing faster convergence.</p></li>
<li><p><code>num_class</code>is a parameter which must be added by yourself for multiclass problems.</p></li>
<li><p><code>enable_feature_grouping</code>is not available in every xgboost version.</p></li>
<li><p><code>sketch_eps</code>because <code>"approx"</code> method is obsolete since <code>"hist"</code> exists.</p></li>
<li><p><code>max_delta_step</code>should be defined by yourself only when you need it (especially for Poisson regression which has exploding gradients).</p></li>
<li><p><code>tweedie_variance_power</code>should be defined by yourself when you are optimizing Tweedie distribution objectives.</p></li>
<li><p><code>updater</code>because we don't expect you to modify the sequence of tree updates, as xgboost automatically defines it.</p></li>
<li><p><code>refresh_leaf</code>because we are not only updating node statistics.</p></li>
<li><p><code>process_type</code>because we let xgboost do its job.</p></li>
<li><p><code>???</code>because I might have missed some other important parameters.</p></li>
</ul>
    <p>You may add them without any issues unlike other parameters.</p>
    

    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'><span class='fu'>library</span>(<span class='no'>Matrix</span>)
<span class='fu'>library</span>(<span class='no'>xgboost</span>)

<span class='fu'>data</span>(<span class='no'>agaricus.train</span>, <span class='kw'>package</span> <span class='kw'>=</span> <span class='st'>"xgboost"</span>)
<span class='fu'>data</span>(<span class='no'>agaricus.test</span>, <span class='kw'>package</span> <span class='kw'>=</span> <span class='st'>"xgboost"</span>)

<span class='no'>dtrain</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='http://www.rdocumentation.org/packages/xgboost/topics/xgb.DMatrix'>xgb.DMatrix</a></span>(<span class='no'>agaricus.train</span>$<span class='no'>data</span>, <span class='kw'>label</span> <span class='kw'>=</span> <span class='no'>agaricus.train</span>$<span class='no'>label</span>)
<span class='no'>dtest</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='http://www.rdocumentation.org/packages/xgboost/topics/xgb.DMatrix'>xgb.DMatrix</a></span>(<span class='no'>agaricus.test</span>$<span class='no'>data</span>, <span class='kw'>label</span> <span class='kw'>=</span> <span class='no'>agaricus.test</span>$<span class='no'>label</span>)
<span class='no'>watchlist</span> <span class='kw'>&lt;-</span> <span class='fu'>list</span>(<span class='kw'>train</span> <span class='kw'>=</span> <span class='no'>dtrain</span>, <span class='kw'>eval</span> <span class='kw'>=</span> <span class='no'>dtest</span>)

<span class='no'>model</span> <span class='kw'>&lt;-</span> <span class='fu'>Laurae.xgb.train</span>(<span class='kw'>train</span> <span class='kw'>=</span> <span class='no'>dtrain</span>,
                          <span class='kw'>watchlist</span> <span class='kw'>=</span> <span class='no'>watchlist</span>,
                          <span class='kw'>verbose</span> <span class='kw'>=</span> <span class='fl'>1</span>,
                          <span class='kw'>objective</span> <span class='kw'>=</span> <span class='st'>"binary:logistic"</span>,
                          <span class='kw'>eval_metric</span> <span class='kw'>=</span> <span class='st'>"auc"</span>,
                          <span class='kw'>tree_depth</span> <span class='kw'>=</span> <span class='fl'>2</span>,
                          <span class='kw'>learn_shrink</span> <span class='kw'>=</span> <span class='fl'>1</span>,
                          <span class='kw'>learn_threads</span> <span class='kw'>=</span> <span class='fl'>1</span>,
                          <span class='kw'>iteration_max</span> <span class='kw'>=</span> <span class='fl'>5</span>)</div><div class='output co'>#&gt; [19:44:30] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.
#&gt; [1]	train-auc:0.958228	eval-auc:0.960373 
#&gt; Multiple eval metrics are present. Will use eval_auc for early stopping.
#&gt; Will train until eval_auc hasn't improved in 20 rounds.
#&gt; 
#&gt; [2]	train-auc:0.987601	eval-auc:0.986900 
#&gt; [3]	train-auc:0.997002	eval-auc:0.998611 
#&gt; [4]	train-auc:0.997409	eval-auc:0.999830 
#&gt; [5]	train-auc:0.998740	eval-auc:0.999915 </div><div class='input'>
</div></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <h2>Contents</h2>
    <ul class="nav nav-pills nav-stacked">
      <li><a href="#arguments">Arguments</a></li>
      
      <li><a href="#value">Value</a></li>

      <li><a href="#details">Details</a></li>
      
      <li><a href="#examples">Examples</a></li>
    </ul>

  </div>
</div>

      <footer>
      <div class="copyright">
  <p>Developed by First Last.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
   </div>

  </body>
</html>
